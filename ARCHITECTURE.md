# ðŸ—ï¸ Talent AI - System Architecture

## Overview

Talent AI is a modern web application built with a decoupled architecture, featuring a Python/FastAPI backend for web scraping and data processing, and a React frontend for user interaction and data visualization.

## System Components

### ðŸŽ¨ Frontend Layer (React)

```
frontend/
â”œâ”€â”€ Components Architecture
â”‚   â”œâ”€â”€ App.js                 # Main application orchestrator
â”‚   â”œâ”€â”€ SearchPanel.js          # Company search & quick-start interface
â”‚   â”œâ”€â”€ JobsTable.js           # Paginated job listings with filtering
â”‚   â””â”€â”€ Analytics.js           # Data visualization dashboard
â”‚
â”œâ”€â”€ State Management
â”‚   â”œâ”€â”€ Local Component State   # useState for UI state
â”‚   â”œâ”€â”€ Props Drilling          # Parent-child data flow
â”‚   â””â”€â”€ Session Storage         # Recent companies cache
â”‚
â””â”€â”€ Styling System
    â”œâ”€â”€ CSS Variables           # Design tokens
    â”œâ”€â”€ Component CSS           # Scoped styles
    â””â”€â”€ Responsive Design       # Mobile-first approach
```

### ðŸ”§ Backend Layer (FastAPI)

```
backend/
â”œâ”€â”€ API Layer
â”‚   â”œâ”€â”€ FastAPI Routes          # RESTful endpoints
â”‚   â”œâ”€â”€ CORS Middleware         # Cross-origin handling
â”‚   â””â”€â”€ Request Validation      # Pydantic models
â”‚
â”œâ”€â”€ Scraping Engine
â”‚   â”œâ”€â”€ Playwright Browser      # Headless Chrome automation
â”‚   â”œâ”€â”€ BeautifulSoup Parser    # HTML extraction
â”‚   â””â”€â”€ Pattern Recognition     # Smart element detection
â”‚
â””â”€â”€ Data Processing
    â”œâ”€â”€ Analytics Engine        # Statistical calculations
    â”œâ”€â”€ Export Formatter        # JSON/CSV generation
    â””â”€â”€ Storage Manager         # File-based persistence
```

## Data Flow Architecture

```mermaid
graph TD
    A[User Interface] -->|HTTP Request| B[FastAPI Server]
    B --> C{Route Handler}
    C -->|/scrape| D[Scraping Engine]
    C -->|/companies| E[Storage Manager]
    C -->|/analytics| F[Analytics Engine]
    C -->|/export| G[Export Formatter]

    D --> H[Playwright Browser]
    H --> I[Greenhouse Site]
    I --> J[HTML Response]
    J --> K[BeautifulSoup Parser]
    K --> L[Data Extraction]
    L --> M[Job Data Model]

    M --> N[Storage]
    M --> F
    F --> O[Statistics]
    O -->|JSON Response| A

    style A fill:#ee7335
    style B fill:#291611
    style H fill:#c7b299
```

## Component Details

### Frontend Architecture

#### Component Hierarchy
```
App.js
â”œâ”€â”€ Header
â”œâ”€â”€ Navigation Tabs
â”œâ”€â”€ SearchPanel
â”‚   â”œâ”€â”€ Search Form
â”‚   â”œâ”€â”€ Quick-Start Buttons
â”‚   â””â”€â”€ Recent Companies
â”œâ”€â”€ JobsTable
â”‚   â”œâ”€â”€ Filter Input
â”‚   â”œâ”€â”€ Table Headers (sortable)
â”‚   â”œâ”€â”€ Job Rows
â”‚   â””â”€â”€ Action Buttons
â””â”€â”€ Analytics
    â”œâ”€â”€ Statistics Cards
    â”œâ”€â”€ Department Chart
    â”œâ”€â”€ Location Chart
    â””â”€â”€ Salary Chart
```

#### State Management Pattern
```javascript
// App.js - Central State
const [jobs, setJobs] = useState([])
const [analytics, setAnalytics] = useState(null)
const [loading, setLoading] = useState(false)
const [activeTab, setActiveTab] = useState('search')

// Props Flow
<SearchPanel onScrape={handleScrape} />
<JobsTable jobs={jobs} />
<Analytics data={analytics} />
```

### Backend Architecture

#### Request Processing Pipeline
```python
# 1. Request Reception
@app.post("/scrape")
async def scrape_endpoint(request: ScrapeRequest):

    # 2. Validation
    validate_greenhouse_url(request.company_url)

    # 3. Scraping Execution
    scraper = TalentScraper()
    await scraper.initialize()
    jobs = await scraper.scrape_greenhouse_jobs(url)

    # 4. Data Processing
    analytics = calculate_analytics(jobs)

    # 5. Storage
    save_to_json(company_name, jobs, analytics)

    # 6. Response
    return ScrapeResponse(jobs=jobs, analytics=analytics)
```

#### Scraping Engine Architecture
```python
class TalentScraper:
    def __init__(self):
        self.browser = None
        self.context = None

    async def scrape_greenhouse_jobs(self, url):
        # 1. Page Navigation
        await page.goto(url)

        # 2. Element Detection
        job_elements = await find_job_elements(page)

        # 3. Data Extraction
        for element in job_elements:
            job = await parse_job_element(element)

            # 4. Detail Fetching (if needed)
            if not job.salary:
                details = await scrape_job_details(job.url)

        return jobs
```

## Technology Stack

### Frontend Stack
- **React 18.2** - UI framework
- **Chart.js 4.x** - Data visualization
- **Axios** - HTTP client
- **CSS3** - Styling with custom properties
- **JetBrains Mono** - Typography

### Backend Stack
- **Python 3.8+** - Core language
- **FastAPI** - Web framework
- **Playwright** - Browser automation
- **BeautifulSoup4** - HTML parsing
- **Pandas** - Data manipulation
- **Uvicorn** - ASGI server

## Performance Optimizations

### Frontend Optimizations
1. **Lazy Loading**: Components load on-demand
2. **Memoization**: Chart configurations cached
3. **Debounced Search**: Filter input debounced
4. **Virtual Scrolling**: Large job lists virtualized

### Backend Optimizations
1. **Concurrent Scraping**: Up to 20 parallel job detail fetches
2. **Connection Pooling**: Reused browser contexts
3. **Smart Caching**: Session-based result storage
4. **Selective Parsing**: Only parse needed elements

## Scalability Considerations

### Horizontal Scaling
```
Load Balancer
    â”œâ”€â”€ FastAPI Instance 1
    â”œâ”€â”€ FastAPI Instance 2
    â””â”€â”€ FastAPI Instance 3
         â””â”€â”€ Shared Storage (Redis/S3)
```

### Vertical Scaling
- Increase concurrent scraping limits
- Expand browser pool size
- Optimize memory usage with streaming

## Security Architecture

### Input Validation
```python
# URL Validation
def validate_greenhouse_url(url: str):
    parsed = urlparse(url)
    if parsed.netloc != 'job-boards.greenhouse.io':
        raise ValueError("Invalid Greenhouse URL")
```

### CORS Configuration
```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3100"],
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)
```

### Data Sanitization
- HTML content stripped before storage
- User inputs validated with Pydantic
- SQL injection prevention (if DB added)

## Deployment Architecture

### Development Environment
```bash
# Backend
uvicorn app:app --reload --host 0.0.0.0 --port 8100

# Frontend
PORT=3100 npm start
```

### Production Environment
```bash
# Docker Compose
services:
  backend:
    build: ./backend
    ports: ["8100:8100"]

  frontend:
    build: ./frontend
    ports: ["80:80"]

  nginx:
    image: nginx
    volumes: ["./nginx.conf:/etc/nginx/nginx.conf"]
```

## Monitoring & Logging

### Application Metrics
- Request/response times
- Scraping success rates
- Error frequencies
- Memory usage

### Logging Strategy
```python
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Structured logging
logger.info(f"Scraping {company_url}", extra={
    "company": company_name,
    "job_count": len(jobs),
    "duration": elapsed_time
})
```

## Future Architecture Enhancements

### Phase 1: Database Integration
```
PostgreSQL Database
â”œâ”€â”€ jobs table
â”œâ”€â”€ companies table
â”œâ”€â”€ analytics table
â””â”€â”€ user_sessions table
```

### Phase 2: Queue System
```
Redis Queue
â”œâ”€â”€ Scraping tasks
â”œâ”€â”€ Export tasks
â””â”€â”€ Analytics calculations
```

### Phase 3: Microservices
```
API Gateway
â”œâ”€â”€ Scraping Service
â”œâ”€â”€ Analytics Service
â”œâ”€â”€ Export Service
â””â”€â”€ Notification Service
```

## Error Handling Strategy

### Frontend Error Boundaries
```javascript
class ErrorBoundary extends React.Component {
  componentDidCatch(error, errorInfo) {
    logErrorToService(error, errorInfo)
  }
}
```

### Backend Error Handling
```python
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error"}
    )
```

## Testing Architecture

### Frontend Testing
- **Unit Tests**: Jest + React Testing Library
- **Integration Tests**: Cypress
- **Visual Regression**: Percy

### Backend Testing
- **Unit Tests**: pytest
- **Integration Tests**: pytest + httpx
- **E2E Tests**: Playwright

## Development Workflow

### Git Branch Strategy
```
main
â”œâ”€â”€ develop
â”‚   â”œâ”€â”€ feature/new-scraper
â”‚   â”œâ”€â”€ feature/analytics-v2
â”‚   â””â”€â”€ fix/salary-parsing
â””â”€â”€ release/v1.1.0
```

### CI/CD Pipeline
```yaml
steps:
  - lint
  - test
  - build
  - deploy-staging
  - deploy-production
```

---

This architecture is designed for maintainability, scalability, and performance while keeping the initial implementation simple and focused.